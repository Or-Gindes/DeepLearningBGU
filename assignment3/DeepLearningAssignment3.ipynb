{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVqjBm1Qxoud"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T10:20:47.520996Z",
     "start_time": "2024-07-06T10:20:47.516795Z"
    },
    "id": "qosNZWs1v02j"
   },
   "outputs": [],
   "source": [
    "#! pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:13:44.442325Z",
     "start_time": "2024-07-06T15:13:40.214524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXhiaI7fsidz",
    "outputId": "613fad3d-f86e-461d-a2c2-29f230c0243b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "import zipfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretty_midi\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Training - lyrics + melody (.mid files with notes, instruments, etc.)\n",
    "# Test - generate lyrics for melody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:13:45.844325Z",
     "start_time": "2024-07-06T15:13:44.443331Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ucLkn-s9vobj",
    "outputId": "aa7d207b-de7f-4072-fe54-991a9e508996"
   },
   "outputs": [],
   "source": [
    "# Get midi files folder and train/test .csv files\n",
    "def download_and_extract(repo_url, output_path):\n",
    "    # check if the output path exists\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    # download ZIP file\n",
    "    response = requests.get(repo_url)\n",
    "    if response.status_code == 200:\n",
    "        # Extract from ZIP file\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "            zip_ref.extractall(output_path)\n",
    "        print(f\"Files extracted to: {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download ZIP file: {response.status_code}\")\n",
    "\n",
    "# Example usage\n",
    "repo_url = \"https://github.com/Or-Gindes/DeepLearningBGU/raw/main/assignment3/Archive.zip?raw=true\"  # link to the ZIP file\n",
    "output_path = \"./\"  # output directory\n",
    "\n",
    "download_and_extract(repo_url, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing MIDIs into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:13:45.857043Z",
     "start_time": "2024-07-06T15:13:45.845339Z"
    },
    "id": "CzIw0Zsg2zCX"
   },
   "outputs": [],
   "source": [
    "class MIDIDataset(Dataset):\n",
    "  \"\"\"\n",
    "  Dataset object for MIDI files.\n",
    "  Attributes:\n",
    "    dataset: pandas dataframe with columns: artist, title, lyricsx\n",
    "    path_to_mids: path to folder with MIDI files\n",
    "  \"\"\"\n",
    "  def __init__(self, path_to_mids: str, dataset: pd.DataFrame):\n",
    "    self.dataset = dataset.copy()\n",
    "    self.parse_dataset(path_to_mids)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.dataset.iloc[idx, :]\n",
    "\n",
    "  def __getattr__(self, name):\n",
    "      if name in self.dataset.columns:\n",
    "          return self.dataset[name]\n",
    "      raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n",
    "\n",
    "  def get_item_from_artist_song_name(self, artist, song_name):\n",
    "    \"\"\"\n",
    "    returns item from dataset based on artist and song name\n",
    "    :param artist: artist name\n",
    "    :param song_name: song name\n",
    "    \"\"\"\n",
    "    return self.dataset.loc[self.dataset['artist'] == artist and self.dataset['title'] == song_name, :]\n",
    "\n",
    "  def parse_dataset(self, path_to_mids):\n",
    "    \"\"\"\n",
    "    Parses dataset and adds:\n",
    "      directory column - which contains path to MIDI file.\n",
    "      parsed_mid column - which contains parsed MIDI data.\n",
    "    \"\"\"\n",
    "    self.dataset['directory'] = (path_to_mids + self.dataset.iloc[:, 0].str.strip() + \"_-_\" + self.dataset.iloc[:, 1].str.strip()).str.strip().str.replace(' ', '_') + \".mid\"\n",
    "    files = []\n",
    "    for file in os.listdir(path_to_mids):\n",
    "      if file.endswith('.mid'):\n",
    "        files.append(os.path.join(path_to_mids, file))\n",
    "    cleaned_file_names = [re.sub(r'_-_live.*|_-_extended.*|_-_violin.*|-2.mid', '.mid', file.lower()) for file in files]\n",
    "\n",
    "    directory_dict = dict(zip(cleaned_file_names, files))\n",
    "    self.dataset['directory'] = [directory_dict.get(song) for song in self.dataset['directory']]\n",
    "    self.dataset['parsed_mid'] = self.dataset['directory'].map(MIDIDataset.get_midi_data)\n",
    "    self.dataset = self.dataset.dropna()\n",
    "    self.dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  @staticmethod\n",
    "  def get_midi_data(path_to_mid: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a path extract features from MIDI file\n",
    "    :param path_to_mid: path to MIDI file\n",
    "    :param start_time: start time in seconds from the begining of the song\n",
    "    :param end_time: end time in seconds from the begining of the song\n",
    "    :return:\n",
    "    \"\"\"\n",
    "      # Load MIDI file, handling potential KeySignatureErrors\n",
    "    try:\n",
    "      midi_data = pretty_midi.PrettyMIDI(path_to_mid)\n",
    "    except Exception as e:\n",
    "      print(f\"Skipping file {path_to_mid}: {e}\")\n",
    "      return None  # or handle the error differently as needed\n",
    "\n",
    "    # remove noise\n",
    "    midi_data.remove_invalid_notes()\n",
    "\n",
    "    # parse midi to df\n",
    "    instrument_col = []\n",
    "    pitch_col = []\n",
    "    velocity_col = []\n",
    "    start_col = []\n",
    "    end_col = []\n",
    "    for i, instrument in enumerate(midi_data.instruments):\n",
    "      for j, note in enumerate(instrument.notes):\n",
    "        start_col.append(note.start)\n",
    "        end_col.append(note.end)\n",
    "        instrument_col.append(MIDIDataset.parse_instrument(instrument.name))\n",
    "        pitch_col.append(note.pitch)\n",
    "        velocity_col.append(note.velocity)\n",
    "\n",
    "    end_col = np.array(end_col)\n",
    "    start_col = np.array(start_col)/np.max(end_col)\n",
    "    end_col /= np.max(end_col)\n",
    "    pitch_col = np.array(pitch_col)/127\n",
    "    velocity_col = np.array(velocity_col)/127\n",
    "\n",
    "    df = pd.DataFrame({\"start\": start_col,\n",
    "                      \"end\": end_col,\n",
    "                      \"instrument\": instrument_col,\n",
    "                      \"duration\": end_col - start_col,\n",
    "                      \"pitch\": pitch_col,\n",
    "                      \"velocity\": velocity_col\n",
    "                      })\n",
    "\n",
    "    df = df.sort_values(by=[\"start\", \"end\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    ohe = OneHotEncoder(categories=[['guitar', 'strings', 'keyboard', 'horns', 'drums', 'melody', 'other']])\n",
    "    ohe_columns = ohe.fit_transform(df[[\"instrument\"]])\n",
    "    df.drop(columns=['instrument'], inplace=True)\n",
    "    df = np.concatenate((df.values, ohe_columns.toarray()), axis=1)\n",
    "    return df\n",
    "\n",
    "  @staticmethod\n",
    "  def parse_instrument(instrument):\n",
    "    if re.search(r'guitar|bass', instrument.lower()):\n",
    "      return 'guitar'\n",
    "    elif re.search(r'violin|cello', instrument.lower()):\n",
    "      return 'strings'\n",
    "    elif re.search(r'piano|keyboard|organ', instrument.lower()):\n",
    "      return 'keyboard'\n",
    "    elif re.search(r'sax|saxophone|trump.*|clarinet|flute', instrument.lower()):\n",
    "      return 'horns'\n",
    "    elif re.search(r'drum.*', instrument.lower()):\n",
    "      return 'drums'\n",
    "    elif re.search(r'melody', instrument.lower()):\n",
    "      return 'melody'\n",
    "    else:\n",
    "      return 'other'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets and creating validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:14:56.008107Z",
     "start_time": "2024-07-06T15:13:45.858049Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "x69sg8qs0CBl",
    "outputId": "b6f33e14-91db-481d-ba4a-89a65bfcd7f3"
   },
   "outputs": [],
   "source": [
    "# Load train and test lyrics\n",
    "def load_lyrics(path: str) -> pd.DataFrame:\n",
    "  df = pd.read_csv(path, header=None, usecols=[0,1,2])\n",
    "  df.columns = [\"artist\", \"title\", \"lyrics\"]\n",
    "  return df\n",
    "\n",
    "# Load train and test MIDI files, create validation set\n",
    "train = load_lyrics(\"lyrics_train_set.csv\")\n",
    "unique_artists = np.array(list(set(train.artist)))\n",
    "number_of_artists_in_validation = int(len(unique_artists) * 0.1)\n",
    "random_choice = np.random.choice(range(number_of_artists_in_validation), number_of_artists_in_validation, replace=False)\n",
    "\n",
    "validation = train[train.artist.isin(unique_artists[random_choice])]\n",
    "validation.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train = train[~train.artist.isin(unique_artists[random_choice])]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train = MIDIDataset(\"./midi_files/\", train)\n",
    "validation = MIDIDataset(\"./midi_files/\", validation)\n",
    "\n",
    "test = load_lyrics(\"lyrics_test_set.csv\")\n",
    "test = MIDIDataset(\"./midi_files/\", test)\n",
    "\n",
    "print(\"------------------------------------------\\n-----------------Train:-------------------\\n------------------------------------------\")\n",
    "display(train.dataset.info())\n",
    "print(\"------------------------------------------\\n-----------------Validation:--------------\\n------------------------------------------\")\n",
    "display(validation.dataset.info())\n",
    "print(\"------------------------------------------\\n-----------------Test:--------------------\\n------------------------------------------\")\n",
    "display(test.dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLMwi9eEIPhM"
   },
   "source": [
    "### Setup DataLoader for the model (+ lyrics preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:14:56.070585Z",
     "start_time": "2024-07-06T15:14:56.009116Z"
    },
    "id": "TK0a6EoZjyY3"
   },
   "outputs": [],
   "source": [
    "def tokenize_lyrics(lyrics: pd.Series):\n",
    "  # Maintain line seperation in songs with end-of-line token (eos)\n",
    "  processed_lyrics = lyrics.apply(lambda song: song.replace(\" & \", \" eol \"))\n",
    "\n",
    "  # Remove punctuation\n",
    "  processed_lyrics = processed_lyrics.apply(lambda song: re.sub(r'[^\\w\\s]', '', song))\n",
    "\n",
    "  # Remove numbers\n",
    "  processed_lyrics = processed_lyrics.apply(lambda song: re.sub(r'\\d+', '', song))\n",
    "\n",
    "  # verify all tokens are lower case letters and strip whitespace\n",
    "  # And add end-of-song token (eos)\n",
    "  # Remove cases where 'eol' token appears twice in a row\n",
    "  eol = \"eol\"\n",
    "  pattern = rf'\\b{re.escape(eol)}\\b\\s+\\b{re.escape(eol)}\\b'\n",
    "\n",
    "  tokens = processed_lyrics.apply(lambda lyrics: [\n",
    "      word.lower() for word in re.sub(pattern, eol, lyrics).strip().split()\n",
    "  ] + [\"eos\"])\n",
    "\n",
    "  return tokens\n",
    "\n",
    "tokens = tokenize_lyrics(train.dataset[\"lyrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:14:56.200850Z",
     "start_time": "2024-07-06T15:14:56.071590Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "1R3fzhY5mCGJ",
    "outputId": "b5948990-02ca-4f40-c547-85bf3d69c6a1"
   },
   "outputs": [],
   "source": [
    "song_word_count = tokens.apply(len)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(song_word_count)\n",
    "quantile_line = song_word_count.quantile(0.9)\n",
    "plt.plot([quantile_line, quantile_line], [0, 300], ':',\n",
    "         label=f\"90% of songs # Words < {round(quantile_line)}\")\n",
    "plt.xlabel(\"# Words\")\n",
    "plt.ylabel(\"# Songs\")\n",
    "plt.title(\"Number of words for songs in training data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pu-473Nna2t"
   },
   "source": [
    "We'll use maximum sequence length = 600 but encourage the model with loss to output songs with around 450 words as this accounts for the overwhelming majority of songs in the corpus.\n",
    "\n",
    "Note that the & sign designating a new line in the song was left the vocabulary to allow the model to reproduce the song structure and the seqence length takes this into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:14:56.682211Z",
     "start_time": "2024-07-06T15:14:56.201855Z"
    },
    "id": "FJRPU7VrfH55"
   },
   "outputs": [],
   "source": [
    "corpus = pd.concat([tokens, tokenize_lyrics(validation.dataset[\"lyrics\"]), tokenize_lyrics(test.dataset[\"lyrics\"])]).reset_index(drop=True)\n",
    "\n",
    "word2vec = Word2Vec(\n",
    "  sentences=corpus,\n",
    "  vector_size=300,\n",
    "  window=5,\n",
    "  min_count=1,\n",
    ")\n",
    "word2vec = word2vec.wv\n",
    "word2vec.save(\"word2vec_lyrics.wordvectors\")\n",
    "\n",
    "word2vec = KeyedVectors.load(\"word2vec_lyrics.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a formatted datasets\n",
    "And seperating melody vectors according to time. We synced the melody to the lyrics by dividing song duration by the number of lyrics, creating a rough estimate of each word’s duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T15:15:01.638499Z",
     "start_time": "2024-07-06T15:14:56.682211Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kusCFfpIUFM",
    "outputId": "b21ccec4-ea8e-4e5e-e25a-2e38e9f0d8bf"
   },
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenized_lyrics: pd.Series(List[str]),\n",
    "        midi_vectors: pd.Series(List[np.array]),\n",
    "        word2vec,\n",
    "        max_seq_length: int = 600,\n",
    "        auto_encoder = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset object for lyrics.\n",
    "        :param tokenized_lyrics: list of string tokens representing song lyrics\n",
    "        :param midi_vectors: preprocess midi vectors\n",
    "        :param word2vec: trained word2vec model\n",
    "        :param max_seq_length: maximum length of lyrics tokens\n",
    "        \"\"\"\n",
    "        self.tokenized_lyrics = tokenized_lyrics\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.auto_encoder = auto_encoder\n",
    "        self.match_seq_length()\n",
    "        self.mid_vectors = []\n",
    "        self.divide_midi(midi_vectors)\n",
    "        self.model = word2vec\n",
    "        self.vectorize_dataset()\n",
    "\n",
    "    def match_seq_length(self):\n",
    "        \"\"\"\n",
    "        Add padding or truncate the lyrics to match max_seq_length\n",
    "        \"\"\"\n",
    "        for ind, lyrics in enumerate(self.tokenized_lyrics):\n",
    "            if len(lyrics) > self.max_seq_length:\n",
    "                lyrics = lyrics[:self.max_seq_length - len(lyrics) - 1] + [\"eos\"]\n",
    "            else:\n",
    "                lyrics += [\"pad\"] * (self.max_seq_length - len(lyrics))\n",
    "            self.tokenized_lyrics[ind] = lyrics\n",
    "\n",
    "    def vectorize_dataset(self):\n",
    "        for ind, lyrics in enumerate(self.tokenized_lyrics):\n",
    "            self.tokenized_lyrics[ind] = [\n",
    "                self.model.key_to_index[token] for token in lyrics if token in self.model.key_to_index\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_lyrics)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.tokenized_lyrics[index]\n",
    "        input_seq = torch.tensor(np.array(tokens[:-1]), dtype=torch.long)\n",
    "        target_seq = torch.tensor(np.array(tokens[1:]), dtype=torch.long)\n",
    "        midi_vector = torch.tensor(np.array(self.mid_vectors[index][1:]), dtype=torch.float32) # to match length of words vectors\n",
    "        return input_seq, target_seq, midi_vector\n",
    "\n",
    "    def divide_midi(self, midi_vectors: pd.Series):\n",
    "        for song_idx, song in self.tokenized_lyrics.items():\n",
    "            duration_of_word = 1 / len(song)\n",
    "            midi = []\n",
    "            for idx, word in enumerate(song):\n",
    "              if (word != \"pad\") and (word != \"eos\"):\n",
    "                midi.append(self.get_midi(parsed_midi=midi_vectors[song_idx], idx=idx, duration=duration_of_word))\n",
    "              else:\n",
    "                midi.append(np.zeros(12))\n",
    "            self.mid_vectors.append(midi)\n",
    "\n",
    "    def replace_midi(self, new_embeddings):\n",
    "          self.mid_vectors = new_embeddings\n",
    "\n",
    "    def get_midi(self, parsed_midi: pd.DataFrame, idx: int, duration:float) -> np.array:\n",
    "        condition = np.logical_and(\n",
    "            (parsed_midi[:, 0] >= float(idx * duration)),\n",
    "            (parsed_midi[:, 1] <= float((idx + 1) * duration))\n",
    "        )\n",
    "        if self.auto_encoder is False:\n",
    "          result = np.mean(parsed_midi[condition], axis=0)\n",
    "          if (len(result) != 12) or (np.isnan(result).any()):\n",
    "            result = np.zeros(12)\n",
    "        else:\n",
    "          result = parsed_midi[condition]\n",
    "          if (len(result) == 0) or (np.isnan(result).any()):\n",
    "            result = np.zeros(12)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "train_dataset = LyricsDataset(\n",
    "    tokenize_lyrics(train.dataset[\"lyrics\"]),\n",
    "    midi_vectors=train.dataset.parsed_mid,\n",
    "    max_seq_length=600,\n",
    "    word2vec=word2vec\n",
    ")\n",
    "\n",
    "validation_dataset = LyricsDataset(\n",
    "    tokenize_lyrics(validation.dataset[\"lyrics\"]),\n",
    "    midi_vectors=validation.dataset.parsed_mid,\n",
    "    max_seq_length=600,\n",
    "    word2vec=word2vec\n",
    ")\n",
    "test_dataset = LyricsDataset(\n",
    "    tokenize_lyrics(test.dataset[\"lyrics\"]),\n",
    "    midi_vectors=test.dataset.parsed_mid,\n",
    "    max_seq_length=600,\n",
    "    word2vec=word2vec\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"There are {len(train_dataset.model)} unique 'words' in the model\\n\"\n",
    "f\"This includes 4 special tokens (end of line, end of song, padding and chorus)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIDIs shape - List[List[np.array]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShaM5yTjfNC0"
   },
   "outputs": [],
   "source": [
    "def print_length(dataset):\n",
    "  print(f\"Dataset length: {len(dataset.mid_vectors)}\")\n",
    "  print(f\"Dataset songs: {len(dataset.mid_vectors[0])}\")\n",
    "  print(f\"Dataset songs vector: {dataset.mid_vectors[0][0].shape}\")\n",
    "  print(f\"Dataset songs vector: {dataset.mid_vectors[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nc609mJbgHBc",
    "outputId": "e5484a17-8ad1-48d9-cd29-8a3a399e633c"
   },
   "outputs": [],
   "source": [
    "print_length(train_dataset)\n",
    "print_length(validation_dataset)\n",
    "print_length(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T17:02:51.162195Z",
     "start_time": "2024-07-06T17:02:51.138898Z"
    },
    "id": "Q0nuPRELpIfp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Losses implementation + CombinedLoss class to return a weighted average\n",
    "\"\"\"\n",
    "\n",
    "def get_song(lyrics: List[str]):\n",
    "    \"\"\"\n",
    "    Helper function that takes a list of strings including special tokens and returns the song as a single string\n",
    "    \"\"\"\n",
    "    song = \" \".join(lyrics).replace(\"eol \", \"\\n\").replace(\"eos\", \"\").strip()\n",
    "    return song\n",
    "\n",
    "\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Cosine Similarity value range is [-1, 1] and Loss value range is [0, 1]\n",
    "    Similarity = 1 => loss = 0 and Similarity = -1 => loss = 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CosineSimilarityLoss, self).__init__()\n",
    "        self.input_type = \"embedding\"\n",
    "        self.similarity = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def forward(self, output, target, not_pad_mask):\n",
    "        res = self.similarity(output, target)\n",
    "        masked_res = res * not_pad_mask\n",
    "        loss = (1 - masked_res.sum() / not_pad_mask.sum()) / 2\n",
    "        return loss\n",
    "\n",
    "class SentimentLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate loss based on a pre-trained Sentiment analysis model\n",
    "    sentiment model polarity scores has compound values [-1, 1],\n",
    "    and negative, positive, neutral values [0, 1].\n",
    "    Sentiment loss per sample in batch are calculated as follows:\n",
    "        sum(MSE(score) per score in [compound, negative, positive, neutral]) / max_error (= 7)\n",
    "    This gives sample loss in range [0, 1]\n",
    "    return average of losses in batch\n",
    "    \"\"\"\n",
    "    def __init__(self, sentiment_model=SentimentIntensityAnalyzer()):\n",
    "        super(SentimentLoss, self).__init__()\n",
    "        self.input_type = \"words\"\n",
    "        self.sid = sentiment_model\n",
    "\n",
    "    def _get_scores(self, sequences):\n",
    "        batch_scores = [\n",
    "            list(self.sid.polarity_scores(\n",
    "                get_song(sequences[seq_ind])\n",
    "            ).values()) for seq_ind in range(sequences.shape[0])\n",
    "        ]\n",
    "        return np.array(batch_scores)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # get score per sample in batch and calculate the average score\n",
    "        output_scores = self._get_scores(output)\n",
    "        target_scores = self._get_scores(target)\n",
    "\n",
    "        # weighted_sentiment_loss range is [0, 2**2 + 3 = 7], divide by 7 for [0, 1] range\n",
    "        sentiment_losses = np.mean(np.sum((output_scores - target_scores) ** 2, axis=1) / 7)\n",
    "        return sentiment_losses\n",
    "\n",
    "\n",
    "class LineLenLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    LineLenLoss: MSE between average line len per song in batch compared to target average line len - loss range [0, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, end_of_line_token=\"eol\", max_seq_len=600):\n",
    "        super(LineLenLoss, self).__init__()\n",
    "        self.input_type = \"words\"\n",
    "        self.end_of_line_token = end_of_line_token\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def _get_avg_line_lens(self, sequences):\n",
    "        eol_mask = (sequences == self.end_of_line_token)\n",
    "        eol_indices = [\n",
    "            np.where(seq_eol_mask)[0] if np.any(seq_eol_mask) else np.array([self.max_seq_len])\n",
    "            for seq_eol_mask in eol_mask\n",
    "        ]\n",
    "        avg_line_len = [\n",
    "            np.mean(np.insert(np.diff(eol_index) - 1, 0, eol_index[0]))\n",
    "            for eol_index in eol_indices\n",
    "        ]\n",
    "        return np.array(avg_line_len)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # line length loss -\n",
    "        output_avg_line_lens = self._get_avg_line_lens(output)\n",
    "        target_avg_line_lens = self._get_avg_line_lens(target)\n",
    "\n",
    "        line_len_loss = np.mean(\n",
    "            abs(output_avg_line_lens - target_avg_line_lens) / self.max_seq_len\n",
    "        )\n",
    "\n",
    "        return line_len_loss\n",
    "\n",
    "\n",
    "class SongLenLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    SongLenLoss: MSE error between song len and target song len - loss range [0, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, end_of_song_token=\"eos\", max_seq_len=600):\n",
    "        super(SongLenLoss, self).__init__()\n",
    "        self.input_type = \"words\"\n",
    "        self.end_of_song_token = end_of_song_token\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def _get_song_len(self, sequences):\n",
    "        eos_mask = (sequences == self.end_of_song_token)\n",
    "        return np.where(np.any(eos_mask, axis=1), np.argmax(eos_mask, axis=1), self.max_seq_len)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # song length loss -\n",
    "        output_song_len = self._get_song_len(output)\n",
    "        target_song_len = self._get_song_len(target)\n",
    "\n",
    "        max_len_error = np.maximum(target_song_len, self.max_seq_len - target_song_len) ** 2\n",
    "        song_len_loss = np.mean((output_song_len - target_song_len) ** 2 / max_len_error)\n",
    "\n",
    "        return song_len_loss\n",
    "\n",
    "\n",
    "class DiversityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Penalize songs which use a small number of unique words to counter repeated word usage\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiversityLoss, self).__init__()\n",
    "        self.input_type = \"words\"\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        unique_word_counts = [len(set(sequence)) for sequence in output]\n",
    "        total_word_counts = [len(sequence) for sequence in output]\n",
    "\n",
    "        # Calculate diversity as the ratio of unique words to total words\n",
    "        diversity_ratios = np.array(unique_word_counts) / np.array(total_word_counts)\n",
    "        diversity_loss = 1 - np.mean(diversity_ratios)\n",
    "\n",
    "        return diversity_loss\n",
    "\n",
    "\n",
    "class EntropyRegularizationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Penalize low entropy in song generation to promote less deterministic word selection\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super(EntropyRegularizationLoss, self).__init__()\n",
    "        self.input_type = \"logits\"\n",
    "        self.max_entropy = np.log(vocab_size)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # Compute softmax to get probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1).mean()\n",
    "        # Normalize entropy by the maximum entropy\n",
    "        normalized_entropy = entropy / self.max_entropy\n",
    "        entropy_loss = 1 - normalized_entropy\n",
    "\n",
    "        return entropy_loss\n",
    "\n",
    "\n",
    "class CombinedLyricsLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word2vec,\n",
    "        loss_func_list: List[nn.Module],\n",
    "        loss_weights: List[float]\n",
    "    ):\n",
    "        super(CombinedLyricsLoss, self).__init__()\n",
    "        self.word2vec = word2vec\n",
    "        self.loss_func_list = loss_func_list\n",
    "        self.loss_weights = loss_weights\n",
    "        self.summed_weights = sum(loss_weights)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.pad_idx = self.word2vec.key_to_index[\"pad\"]\n",
    "\n",
    "    def forward(self, logits, target, embedding_layer):\n",
    "        device = logits.device\n",
    "        batch_size, seq_len, vocab_size = logits.size()\n",
    "\n",
    "        # Calculate CrossEntropyLoss\n",
    "        ce_loss = self.cross_entropy(logits.view(-1, vocab_size), target.view(-1))\n",
    "\n",
    "        # Get predicted words\n",
    "        predicted_indices = torch.argmax(logits, dim=-1)\n",
    "        predicted_words = np.array([\n",
    "            [self.word2vec.index_to_key[idx.item()]\n",
    "             for idx in sequence] for sequence in predicted_indices\n",
    "        ])\n",
    "\n",
    "        # Get target words\n",
    "        target_words = np.array([\n",
    "            [self.word2vec.index_to_key[idx.item()] for idx in sequence]\n",
    "            for sequence in target\n",
    "        ])\n",
    "\n",
    "        # Get embeddings for predicted and target words\n",
    "        predicted_embeddings = embedding_layer(predicted_indices)\n",
    "        target_embeddings = embedding_layer(target)\n",
    "        not_pad_mask = (target != self.pad_idx)\n",
    "\n",
    "        weighted_loss = ce_loss.clone()\n",
    "\n",
    "        for ind, loss_func in enumerate(self.loss_func_list):\n",
    "            if self.loss_weights[ind] == 0:\n",
    "                continue\n",
    "            if loss_func.input_type == \"words\":\n",
    "                loss = torch.tensor(loss_func(predicted_words, target_words), device=device)\n",
    "            elif loss_func.input_type == \"embedding\":\n",
    "                loss = loss_func(predicted_embeddings, target_embeddings, not_pad_mask)\n",
    "            elif loss_func.input_type == \"logits\":\n",
    "                loss = loss_func(logits, target)\n",
    "            else:\n",
    "                raise AssertionError(\"Loss function is missing input_type attribute\")\n",
    "\n",
    "            weighted_loss += self.loss_weights[ind] * loss\n",
    "\n",
    "        return weighted_loss / (self.summed_weights + 1)  # +1 for CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:48:37.193230Z",
     "start_time": "2024-07-06T18:48:37.166828Z"
    },
    "id": "0O0e8XRiQa_1"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class LyricsGAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word2vec,\n",
    "        hidden_dim: int = 256,\n",
    "        melody_hidden_dim: int = 128,\n",
    "        rnn_type: str = \"gru\",\n",
    "        n_rnn_layers: int = 3,\n",
    "        dropout: float = 0.6,\n",
    "        use_attention: bool = False,\n",
    "        criterion = nn.CrossEntropyLoss(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generative Lyrics model including train and song generation methods\n",
    "        :param word2vec: an initial word2vec model to use for the embedding layer\n",
    "        :param melody_hidden_dim: hidden size of the melody layer\n",
    "        :param hidden_dim: number of hidden nodes in the RNN architecture\n",
    "        :param rnn_type: type of RNN cell (\"lstm\" or \"gru\")\n",
    "        :param n_rnn_layers: Number of RNN layers in the RNN block\n",
    "        :param dropout: dropout probability in the RNN cell\n",
    "        :param use_attention: whether to use attention mechanism\n",
    "        :param criterion: Loss function to use in training\n",
    "        \"\"\"\n",
    "        super(LyricsGAN, self).__init__()\n",
    "\n",
    "        self.word2vec_model = word2vec\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = criterion.to(self.device)\n",
    "        self.use_attention = use_attention\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.melody_hidden_dim = melody_hidden_dim\n",
    "        self.writer = SummaryWriter()\n",
    "        self.vector_size = word2vec.vector_size\n",
    "        self.vocab_size = len(word2vec)\n",
    "\n",
    "        embedding_matrix = np.zeros((self.vocab_size, self.vector_size))\n",
    "        for word, idx in word2vec.key_to_index.items():\n",
    "            embedding_matrix[idx] = word2vec[word]\n",
    "\n",
    "        embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "        self.embedding = nn.Sequential(nn.Embedding.from_pretrained(embedding_matrix, freeze=True), nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "        # The \"encoding\" layer of the melody input\n",
    "        self.melody_encoder = nn.Sequential(\n",
    "            nn.Linear(12, melody_hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        if self.use_attention:\n",
    "            # attention mechanisms for melody input\n",
    "            self.melody_attention = nn.MultiheadAttention(melody_hidden_dim, num_heads=1, batch_first=True)\n",
    "            # self-attention for sequence\n",
    "            self.word_attention = nn.MultiheadAttention(self.vector_size, num_heads=1, batch_first=True)\n",
    "\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn_block = nn.LSTM(\n",
    "                input_size=self.vector_size + melody_hidden_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_rnn_layers,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        else:\n",
    "            self.rnn_block = nn.GRU(\n",
    "                input_size=self.vector_size + melody_hidden_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_rnn_layers,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dense = nn.Linear(hidden_dim, self.vocab_size)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x, melody_state, prev_state=None):\n",
    "        b_size, seq_len = x.size()\n",
    "        x_embbed = self.embedding(x)\n",
    "        melody_state = self.melody_encoder(melody_state)\n",
    "\n",
    "        if self.use_attention:\n",
    "            if prev_state is None:\n",
    "                query = torch.zeros(b_size, 1, self.hidden_dim).to(self.device)\n",
    "            else:\n",
    "                query = prev_state[0][-1].view(b_size, 1, self.melody_hidden_dim)\n",
    "\n",
    "            melody_context, melody_attention = self.melody_attention(query, melody_state, melody_state)\n",
    "            melody_context = melody_context.repeat(1, seq_len, 1)\n",
    "\n",
    "            # mask future words so model won't rely on them when generating\n",
    "            causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(self.device)\n",
    "            word_context, word_attention = self.word_attention(x_embbed, x_embbed, x_embbed, attn_mask=causal_mask)\n",
    "\n",
    "            combined_input = torch.cat((word_context, melody_context), dim=2)\n",
    "        else:\n",
    "            combined_input = torch.cat((x_embbed, melody_state), dim=2)\n",
    "\n",
    "        output, state = self.rnn_block(combined_input, prev_state)\n",
    "\n",
    "        output = self.bn(output.transpose(1, 2)).transpose(1, 2)  # apply batchnorm to the feature space\n",
    "        logits = self.dense(output)\n",
    "        return logits, state\n",
    "\n",
    "    def train_model(\n",
    "        self,\n",
    "        train_loader,\n",
    "        validation_loader,\n",
    "        epochs: int,\n",
    "        patience: int = 10,\n",
    "        lr: float = 0.001\n",
    "    ):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        self.train()\n",
    "        losses = {\"Train\": [], \"Validation\": []}\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        steps = 1\n",
    "        with tqdm(total=epochs, desc='Training') as pbar:\n",
    "            for epoch in range(epochs):\n",
    "                total_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "                # Iterate over the train_loader (batches)\n",
    "                for inputs, targets, midi_vectors in train_loader:\n",
    "                    steps += 1\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "                    midi_vectors = midi_vectors.to(self.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    logits, _ = self(inputs, midi_vectors)\n",
    "\n",
    "                    loss = combined_loss(logits, targets, model.embedding)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss\n",
    "\n",
    "                avg_train_loss = total_loss.item() / len(train_loader)\n",
    "                # Log training loss to TensorBoard\n",
    "                self.writer.add_scalar('Loss/train_step', loss.item(), steps)\n",
    "                \n",
    "                # Validation loop\n",
    "                self.eval()\n",
    "                total_val_loss = torch.tensor(0.0, device=self.device)\n",
    "                with torch.no_grad():\n",
    "                    for inputs, targets, midi_vectors in validation_loader:\n",
    "                        inputs = inputs.to(self.device)\n",
    "                        targets = targets.to(self.device)\n",
    "                        midi_vectors = midi_vectors.to(self.device)\n",
    "\n",
    "                        logits, _ = self(inputs, midi_vectors)\n",
    "\n",
    "                        val_loss = combined_loss(logits, targets, model.embedding)\n",
    "                        total_val_loss += val_loss\n",
    "\n",
    "                avg_val_loss = total_val_loss.item() / len(validation_loader)\n",
    "\n",
    "                # Log epoch losses to TensorBoard\n",
    "                self.writer.add_scalars('Loss/epoch', {\n",
    "                'train': avg_train_loss,\n",
    "                'validation': avg_val_loss\n",
    "                }, epoch)\n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_epoch = epoch + 1\n",
    "                    patience_counter = 0\n",
    "                    # Save the best model\n",
    "                    torch.save(self.state_dict(), 'best_model.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f'Epoch {epoch+1}/{epochs}, '\n",
    "                    f'Train Loss: {round(avg_train_loss, 4)}, '\n",
    "                    f'Val Loss: {round(avg_val_loss, 4)}, '\n",
    "                    f'BestVal Loss: {round(best_val_loss, 4)} (epoch {best_epoch})'\n",
    "                )\n",
    "                pbar.update(1)\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(\n",
    "                        f'\\nEarly stopping triggered after {epoch + 1} '\n",
    "                        f'epochs with best val loss = {round(best_val_loss, 4)}'\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                if epoch % 25 == 0:\n",
    "                    save_path = f'model_epoch_{epoch}.pth'\n",
    "                    torch.save(self.state_dict(), save_path)\n",
    "\n",
    "                    for i in range(3):\n",
    "                      # Choose a random initial word and MIDI vector from the validation dataset to start a song\n",
    "                      random_index = random.randint(0, len(validation_loader.dataset) - 1)\n",
    "                      song_seed, _, melody = validation_loader.dataset[random_index]\n",
    "                      song_seed = validation_loader.dataset.model.index_to_key[song_seed[0].item()]\n",
    "                      generated_song = self.generate_song(song_seed, melody)\n",
    "                      print(f\"\\nEpoch {epoch} - \"\n",
    "                            f\"Generated Song (probabilistic) #{i + 1} - \"\n",
    "                            f\"word count = {len(generated_song)}:\"\n",
    "                            f\"\\n{get_song(generated_song)}\")\n",
    "\n",
    "                # Set back to train mode\n",
    "                self.train()\n",
    "\n",
    "        # Close the TensorBoard writer\n",
    "        self.writer.close()\n",
    "        \n",
    "    def predict(self, input_seq, midi_vector, prev_state=None):\n",
    "        \"\"\"\n",
    "        Predict method adds a dimension of probability to the predictions\n",
    "        by sampling from the output logits.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        input_seq = input_seq.to(self.device)\n",
    "        midi_vector = midi_vector.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits, state = self(input_seq, midi_vector, prev_state=prev_state)\n",
    "\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        next_word_index = torch.multinomial(probabilities[0, -1], 1).item()\n",
    "\n",
    "        return next_word_index, state\n",
    "\n",
    "    def generate_song(self, initial_words: str, melody, max_length=600):\n",
    "        \"\"\"\n",
    "        Support method to generate songs word-by-word starting from an initial word(s) input\n",
    "        \"\"\"\n",
    "        generated_song = initial_words.split()\n",
    "        current_indices = [\n",
    "            self.word2vec_model.key_to_index[word] for word in generated_song\n",
    "        ]\n",
    "        current_indices = torch.tensor(np.array(current_indices)).unsqueeze(0)\n",
    "        prev_state = None\n",
    "\n",
    "        for i in range(max_length - len(generated_song)):\n",
    "            current_midi = melody[i].unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "            next_word_index, state = self.predict(current_indices, current_midi, prev_state)\n",
    "\n",
    "            next_word = self.word2vec_model.index_to_key[next_word_index]\n",
    "\n",
    "            # if the predicted word is end-of-song token - stop the generation\n",
    "            if next_word == \"eos\":\n",
    "                break\n",
    "\n",
    "            # Add the generated word to the song\n",
    "            generated_song.append(next_word)\n",
    "\n",
    "            # Predict the next word using the last word & the previous state\n",
    "            current_indices = torch.tensor([next_word_index]).unsqueeze(0).to(self.device)\n",
    "            prev_state = state\n",
    "\n",
    "        return generated_song\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T18:57:03.418062Z",
     "start_time": "2024-07-06T18:48:39.538168Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llHWwI-s0Fc5",
    "outputId": "821dbc07-339e-4976-b938-aa7fab4a361f"
   },
   "outputs": [],
   "source": [
    "combined_loss = CombinedLyricsLoss(\n",
    "    word2vec=word2vec,\n",
    "    loss_func_list=[\n",
    "        CosineSimilarityLoss(),\n",
    "        SentimentLoss(),\n",
    "        LineLenLoss(),\n",
    "        SongLenLoss(),\n",
    "        DiversityLoss(),\n",
    "        EntropyRegularizationLoss(vocab_size=(len(word2vec))),\n",
    "    ],\n",
    "    loss_weights=[0.5, 0.1, 0, 0.2, 0.2, 0.1]\n",
    ")\n",
    "\n",
    "model = LyricsGAN(word2vec=word2vec, criterion=combined_loss)\n",
    "model.train_model(\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T17:47:07.402381Z",
     "start_time": "2024-07-06T17:47:07.392123Z"
    },
    "id": "Ws2z9UzbAyYS"
   },
   "outputs": [],
   "source": [
    "def get_song_structure(song):\n",
    "    line_lengths = [len(line.split()) for line in get_song(song).split('\\n')]\n",
    "    return len(line_lengths), np.mean(line_lengths) if line_lengths else 0\n",
    "\n",
    "\n",
    "def get_diversity_score(sequence):\n",
    "    unique_words = len(set(sequence))\n",
    "    total_words = len(sequence)\n",
    "    diversity_ratio = unique_words / total_words\n",
    "    return diversity_ratio\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, num_generations=5):\n",
    "    model.eval()\n",
    "    cosine_sim = nn.CosineSimilarity(dim=0)\n",
    "    sentiment_loss = SentimentLoss()\n",
    "\n",
    "    total_sentiment_similarity = 0\n",
    "    total_cosine_similarity = 0\n",
    "    total_structure_similarity = 0\n",
    "    total_diversity_similarity = 0\n",
    "    total_songs = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _, midi_vector in dataloader:\n",
    "            inputs = inputs.to(model.device)\n",
    "            midi_vector = midi_vector.to(model.device)\n",
    "\n",
    "            original_song = [model.word2vec_model.index_to_key[idx.item()] for idx in inputs[0]] + [\"pad\"]\n",
    "            first_word = original_song[0]\n",
    "\n",
    "            song_sentiment_similarity = 0.0\n",
    "            song_cosine_similarity = 0.0\n",
    "            song_structure_similarity = 0.0\n",
    "            song_diversity_similarity = 0.0\n",
    "\n",
    "            for _ in range(num_generations):\n",
    "                generated_song = model.generate_song(first_word, midi_vector[0])\n",
    "                generated_song += [\"pad\"] * (len(original_song) - len(generated_song))\n",
    "\n",
    "                # Calculate sentiment similarity using SentimentLoss\n",
    "                sentiment_similarity = 1 - sentiment_loss(\n",
    "                    np.array([generated_song]), np.array([original_song])\n",
    "                )\n",
    "                song_sentiment_similarity += sentiment_similarity\n",
    "\n",
    "                # Calculate cosine similarity\n",
    "                generated_indices = [\n",
    "                    model.word2vec_model.key_to_index[word]\n",
    "                    for word in generated_song if word in model.word2vec_model.key_to_index\n",
    "                ]\n",
    "                generated_vec = model.embedding(\n",
    "                    torch.tensor(generated_indices).to(model.device)\n",
    "                )\n",
    "                original_vec = model.embedding(torch.cat((inputs, inputs[:, -1].unsqueeze(1)), dim=-1))\n",
    "\n",
    "                cosine_similarity = torch.mean(cosine_sim(generated_vec, original_vec.squeeze(0))).item()\n",
    "                song_cosine_similarity += cosine_similarity\n",
    "\n",
    "                # Calculate structure similarity\n",
    "                gen_song_len, gen_avg_line_len = get_song_structure(generated_song)\n",
    "                orig_song_len, orig_avg_line_len = get_song_structure(original_song)\n",
    "\n",
    "                # Calculate diversity similarity\n",
    "                gen_diversity = get_diversity_score(generated_song)\n",
    "                orig_diversity = get_diversity_score(original_song)\n",
    "                diversity_similarity = 1 - abs(gen_diversity - orig_diversity) / max(gen_diversity, orig_diversity)\n",
    "                song_diversity_similarity += diversity_similarity\n",
    "\n",
    "                song_len_similarity = 1 - abs(gen_song_len - orig_song_len) / max(gen_song_len, orig_song_len)\n",
    "                line_len_similarity = 1 - abs(gen_avg_line_len - orig_avg_line_len) / max(gen_avg_line_len, orig_avg_line_len)\n",
    "                structure_similarity = (song_len_similarity + line_len_similarity) / 2\n",
    "                song_structure_similarity += structure_similarity\n",
    "\n",
    "            # Calculate average similarities for this song and add to the total score\n",
    "            avg_song_sentiment_similarity = song_sentiment_similarity / num_generations\n",
    "            avg_song_cosine_similarity = song_cosine_similarity / num_generations\n",
    "            avg_song_structure_similarity = song_structure_similarity / num_generations\n",
    "            avg_song_diversity_similarity = song_diversity_similarity / num_generations\n",
    "\n",
    "            total_sentiment_similarity += avg_song_sentiment_similarity\n",
    "            total_cosine_similarity += avg_song_cosine_similarity\n",
    "            total_structure_similarity += avg_song_structure_similarity\n",
    "            total_diversity_similarity += avg_song_diversity_similarity\n",
    "\n",
    "    # Calculate overall averages for the dataset\n",
    "    avg_sentiment_similarity = total_sentiment_similarity / total_songs\n",
    "    avg_cosine_similarity = total_cosine_similarity / total_songs\n",
    "    avg_structure_similarity = total_structure_similarity / total_songs\n",
    "    avg_diversity_similarity = total_diversity_similarity / total_songs\n",
    "\n",
    "    print(f\"Average Sentiment Similarity: {avg_sentiment_similarity:.4f}\")\n",
    "    print(f\"Average Cosine Similarity: {avg_cosine_similarity:.4f}\")\n",
    "    print(f\"Average Structure Similarity: {avg_structure_similarity:.4f}\")\n",
    "    print(f\"Average Diversity Similarity: {avg_diversity_similarity:.4f}\")\n",
    "\n",
    "    return avg_sentiment_similarity, avg_cosine_similarity, avg_structure_similarity, avg_diversity_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:14:06.292530Z",
     "start_time": "2024-07-06T19:13:57.134625Z"
    },
    "id": "weI3wuxcObwP"
   },
   "outputs": [],
   "source": [
    "model = LyricsGAN(word2vec=word2vec, criterion=combined_loss)\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T19:15:40.644854Z",
     "start_time": "2024-07-06T19:15:38.397065Z"
    },
    "id": "0AHkEM8jzkj4"
   },
   "outputs": [],
   "source": [
    "song_idx = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, _, midi_vector in test_dataloader:\n",
    "        inputs = inputs.to(model.device)\n",
    "        midi_vector = midi_vector.to(model.device)\n",
    "\n",
    "        original_song = [model.word2vec_model.index_to_key[idx.item()] for idx in inputs[0]]\n",
    "        first_word = original_song[0]\n",
    "        print(f\"\\nLyrics #{song_idx + 1} - Title: {test.dataset.title[song_idx]}\\nSeed: {first_word}\\n\")\n",
    "        generated_song = model.generate_song(first_word, midi_vector[0])\n",
    "        print(get_song(generated_song))\n",
    "        song_idx += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHDQc3mUNYx"
   },
   "source": [
    "# Second method to implement melody integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43wNmL07zkj5"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size: int = 12 * 7, encoding_dim: int = 12):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            #nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, self.encoding_dim),\n",
    "            #nn.BatchNorm1d(self.encoding_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.encoding_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, self.input_size),\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def train_model(self, train_dataset, validation_dataset, num_epochs=100, patience=5):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.003)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        best_model = None\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            train_loss = 0.0\n",
    "            for data, _ in train_dataset:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(data)\n",
    "                loss = criterion(outputs, data)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_loss /= len(train_dataset)\n",
    "\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data, _ in validation_dataset:\n",
    "                    data = data.to(self.device)\n",
    "                    outputs = self(data)\n",
    "                    loss = criterion(outputs, data)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(validation_dataset)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "            if round(val_loss, 4) < round(best_val_loss, 4):\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "                best_model = self.state_dict().copy()\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping after epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "        # Load the best model after training\n",
    "        if best_model is not None:\n",
    "            self.load_state_dict(best_model)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def transform_data(self, dataset, dim_1, dim_2, dim_3):\n",
    "        self.eval()\n",
    "        song_embeddings = []\n",
    "        # Convert all midis to embeddings\n",
    "        with torch.no_grad():\n",
    "            for sublist, _ in dataset:\n",
    "                for subsublist in sublist:\n",
    "                    subsublist = torch.Tensor(subsublist).to(device)\n",
    "                    encoded_representation = self.encoder(subsublist).cpu().numpy()\n",
    "                    song_embeddings.append(encoded_representation)\n",
    "        # Flatten and reshape to proper shape\n",
    "        song_embeddings = np.concatenate([sublist for sublist in song_embeddings])\n",
    "        song_embeddings = song_embeddings.reshape(dim_1, dim_2, dim_3)\n",
    "        song_embeddings = song_embeddings.tolist()\n",
    "        # return to structure list[list[np.array]]\n",
    "        for i in range(len(song_embeddings)):\n",
    "            for j in range(len(song_embeddings[i])):\n",
    "                song_embeddings[i][j] = np.array(song_embeddings[i][j])\n",
    "        return song_embeddings\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_and_flatten_arrays(data, max_len=12*7):\n",
    "        list_of_padded_arrays = []\n",
    "        for list_item in data:\n",
    "            padded_arrays = []\n",
    "            for sublist in list_item:\n",
    "                if sublist.shape == (12,):\n",
    "                    concatenated = sublist\n",
    "                else:\n",
    "                    concatenated = np.concatenate(sublist)\n",
    "                if len(concatenated) < max_len:\n",
    "                    padded_item = np.pad(concatenated, (0, max_len - len(concatenated)), 'constant')\n",
    "                else:\n",
    "                    padded_item = concatenated[:max_len]\n",
    "                padded_arrays.append(padded_item)\n",
    "            list_of_padded_arrays.append(padded_arrays)\n",
    "        return np.array(list_of_padded_arrays, dtype=np.float32)\n",
    "\n",
    "    def prepare_data(self, dataset, batch_size):\n",
    "        transform_dataset = self.pad_and_flatten_arrays(dataset.mid_vectors)\n",
    "        transform_dataset = torch.tensor(transform_dataset).view(-1, self.input_size)\n",
    "        transform_dataset = torch.utils.data.TensorDataset(transform_dataset, transform_dataset)\n",
    "        dataloader = torch.utils.data.DataLoader(transform_dataset, batch_size=batch_size, shuffle=True)\n",
    "        return dataloader\n",
    "\n",
    "    def pipeline(self, train_dataset, val_dataset, test_dataset):\n",
    "        print('Auto-Encoder Activated, preparing datasets: ', end=' ')\n",
    "        train_data = self.prepare_data(dataset=train_dataset, batch_size=16)\n",
    "        val_data = self.prepare_data(val_dataset, batch_size=16)\n",
    "        print('Done.\\nTraining model:\\n')\n",
    "        self.train_model(train_dataset=train_data, validation_dataset=val_data)\n",
    "        print('Done.\\nFinishing Transformation of Data: ', end=' ')\n",
    "        train_transformed = self.transform_data(train_data,\n",
    "                                                537,\n",
    "                                                600,\n",
    "                                                12)\n",
    "\n",
    "        val_transformed = self.transform_data(val_data,\n",
    "                                              57,\n",
    "                                              600,\n",
    "                                              12)\n",
    "        test_data = self.prepare_data(test_dataset, batch_size=16)\n",
    "        test_transformed = self.transform_data(test_data,\n",
    "                                               5,\n",
    "                                               600,\n",
    "                                               12)\n",
    "        print('Done!')\n",
    "        return train_transformed, val_transformed, test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "9vKIvFb4xBOm",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset_autoencoder = LyricsDataset(\n",
    "    tokenize_lyrics(train.dataset[\"lyrics\"]),\n",
    "    midi_vectors=train.dataset.parsed_mid,\n",
    "    max_seq_length=600,\n",
    "    word2vec=word2vec,\n",
    "    auto_encoder=True\n",
    ")\n",
    "\n",
    "validation_dataset_autoencoder = LyricsDataset(\n",
    "    tokenize_lyrics(validation.dataset[\"lyrics\"]),\n",
    "    midi_vectors=validation.dataset.parsed_mid,\n",
    "    max_seq_length=600,\n",
    "    word2vec=word2vec,\n",
    "    auto_encoder=True\n",
    ")\n",
    "test_dataset_autoencoder = LyricsDataset(\n",
    "    tokenize_lyrics(test.dataset[\"lyrics\"]),\n",
    "    midi_vectors=test.dataset.parsed_mid,\n",
    "    max_seq_length=600,\n",
    "    word2vec=word2vec,\n",
    "    auto_encoder=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = Autoencoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "flag = False\n",
    "for song in train_dataset_autoencoder.mid_vectors:\n",
    "    for lyric in song:\n",
    "        len_array = lyric.shape[0]\n",
    "        if len_array == 12:\n",
    "            len_array = 1\n",
    "        lst.append(len_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "editable": true,
    "id": "g4ZFyPooDzc4",
    "outputId": "2310d270-848a-4a17-89d8-90edf4962beb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(lst, bins=56, edgecolor='black')\n",
    "quantile_line = pd.Series(lst).quantile(0.9)\n",
    "plt.plot([quantile_line, quantile_line], [0, max(plt.gca().get_ylim())], ':',\n",
    "         label=f\"90% of songs # Words <= {round(quantile_line)}\")\n",
    "plt.xlabel(\"# Melody Vectors per Lyric\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Number of Melody Vectors per Lyric in Training Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train auto-encoder, replace midis with encoder's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "_TseQJHBAQBY",
    "outputId": "6bfe655a-aba4-45b3-f83f-402dcc972477"
   },
   "outputs": [],
   "source": [
    "train_autoencoded_midis, val_autoencoded_midis, test_autoencoded_midis = autoencoder.pipeline(train_dataset_autoencoder,\n",
    "                                                                                              validation_dataset_autoencoder,\n",
    "                                                                                              test_dataset_autoencoder)\n",
    "\n",
    "train_dataset_autoencoder.replace_midi(train_autoencoded_midis)\n",
    "validation_dataset_autoencoder.replace_midi(val_autoencoded_midis)\n",
    "test_dataset_autoencoder.replace_midi(test_autoencoded_midis)\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset_autoencoder, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset_autoencoder, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset_autoencoder, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"There are {len(train_dataset_autoencoder.model)} unique 'words' in the model\\n\"\n",
    "f\"This includes 4 special tokens (end of line, end of song, padding and chorus)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUtZjufBo9WN"
   },
   "outputs": [],
   "source": [
    "combined_loss = CombinedLyricsLoss(\n",
    "    word2vec=word2vec,\n",
    "    loss_func_list=[\n",
    "        CosineSimilarityLoss(),\n",
    "        SentimentLoss(),\n",
    "        LineLenLoss(),\n",
    "        SongLenLoss(),\n",
    "        DiversityLoss(),\n",
    "        EntropyRegularizationLoss(vocab_size=(len(word2vec))),\n",
    "    ],\n",
    "    loss_weights=[0.5, 0.1, 0, 0.2, 0.2, 0.1]\n",
    ")\n",
    "\n",
    "model = LyricsGAN(word2vec=word2vec, criterion=combined_loss)\n",
    "model.train_model(\n",
    "    train_dataloader,\n",
    "    validation_dataloader,\n",
    "    epochs=100,\n",
    "    lr=0.001,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wJiZcLUpGRP"
   },
   "outputs": [],
   "source": [
    "model = LyricsGAN(word2vec=word2vec, criterion=combined_loss)\n",
    "checkpoint = torch.load(\"best_model.pth\")\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "evaluate_model(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_idx = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, _, midi_vector in test_dataloader:\n",
    "        inputs = inputs.to(model.device)\n",
    "        midi_vector = midi_vector.to(model.device)\n",
    "\n",
    "        original_song = [model.word2vec_model.index_to_key[idx.item()] for idx in inputs[0]]\n",
    "        first_word = 'sad'#original_song[0]\n",
    "        print(f\"\\nLyrics #{song_idx + 1} - Title: {test.dataset.title[song_idx]}\\nSeed: {first_word}\\n\")\n",
    "        generated_song = model.generate_song(first_word, midi_vector[song_idx])\n",
    "        print(get_song(generated_song))\n",
    "        song_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
